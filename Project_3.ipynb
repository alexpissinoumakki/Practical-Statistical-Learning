{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4d914ee",
   "metadata": {},
   "source": [
    "## Load Libraries and Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d27108e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: Matrix\n",
      "\n",
      "Loaded glmnet 4.1-4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(\"text2vec\")\n",
    "library(\"glmnet\")\n",
    "library(\"slam\")\n",
    "\n",
    "set.seed(1528)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5519131c",
   "metadata": {},
   "source": [
    "## Initial Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1da86",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "First, we'll load the entire dataset containing all movie reviews in the `alldata.tsv` file. We'll do additional post-processing to the `\"review\"` column to remove HTML tags (contained within `<>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4b04697",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read.table(\"alldata.tsv\",\n",
    "                  stringsAsFactors = FALSE,\n",
    "                  header = TRUE)\n",
    "data$review = gsub('<.*?>', ' ', data$review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99619476",
   "metadata": {},
   "source": [
    "### Initial DocumentTerm Matrix\n",
    "\n",
    "We use the R package `text2vec` to construct the $DT$ (DocumentTerm) matrix with a maximum of 4-grams allowed. As a preprocessing step, we lowercase all the text and use the `word_tokenizer` to tokenize the words in each review. When creating the vocabulary, we make sure to filter out a pre-defined set of stop words. We prune the vocabulary to filter out rare tokens (< 10 occurrences over all documents) and those that appear in less than $.1\\%$ or more than $50\\%$ of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "537d36f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "as(<dgTMatrix>, \"dgCMatrix\") is deprecated since Matrix 1.5-0; do as(., \"CsparseMatrix\") instead\n",
      "\n"
     ]
    }
   ],
   "source": [
    "it_data = itoken(data$review,\n",
    "                 preprocessor = tolower,\n",
    "                 tokenizer = word_tokenizer)\n",
    "stop_words = c(\"i\", \"me\", \"my\", \"myself\", \n",
    "               \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "               \"you\", \"your\", \"yours\", \n",
    "               \"their\", \"they\", \"his\", \"her\", \n",
    "               \"she\", \"he\", \"a\", \"an\", \"and\",\n",
    "               \"is\", \"was\", \"are\", \"were\", \n",
    "               \"him\", \"himself\", \"has\", \"have\", \n",
    "               \"it\", \"its\", \"the\", \"us\")\n",
    "tmp.vocab = create_vocabulary(it_data,\n",
    "                              stopwords = stop_words, \n",
    "                              ngram = c(1L, 4L))\n",
    "tmp.vocab = prune_vocabulary(tmp.vocab, term_count_min = 10,\n",
    "                             doc_proportion_max = 0.5,\n",
    "                             doc_proportion_min = 0.001)\n",
    "dtm_data = create_dtm(it_data, vocab_vectorizer(tmp.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb85cb",
   "metadata": {},
   "source": [
    "As expected the size of the $DT$ matrix is larger than the vocabulary size (i.e., # of columns of `dtm_train`) is greater than 30,0000 which is bigger than the sample size `n = 25000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d3d030d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>50000</li><li>31590</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 50000\n",
       "\\item 31590\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 50000\n",
       "2. 31590\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 50000 31590"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(dtm_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ca783f",
   "metadata": {},
   "source": [
    "## Improve Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81b67c9",
   "metadata": {},
   "source": [
    "To improve the interpretability of the vocabulary, we apply a simple screening method using the **two-sample t-test**. That is, we consider two groups: (1) *positive*, and (2) *negative*. Then, we compute the $t$-statistics for each word across the two groups and take only the top $k$ words with the largest absolute (i.e., magnitude) $t$-statistics. This way, we hope that the chosen words in the final vocabulary have more meaningful contribution (i.e., are the \"most negative\" or \"most positive\" words) to the final sentiment and make the final model more interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d20580b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.size = dim(dtm_data)[2]\n",
    "labels = data$sentiment\n",
    "\n",
    "summ = matrix(0, nrow=v.size, ncol=4)\n",
    "summ[, 1] = colapply_simple_triplet_matrix(\n",
    "  as.simple_triplet_matrix(dtm_data[labels==1,]), mean)\n",
    "summ[, 2] = colapply_simple_triplet_matrix(\n",
    "  as.simple_triplet_matrix(dtm_data[labels==1,]), var)\n",
    "summ[, 3] = colapply_simple_triplet_matrix(\n",
    "  as.simple_triplet_matrix(dtm_data[labels==0,]), mean)\n",
    "summ[, 4] = colapply_simple_triplet_matrix(\n",
    "  as.simple_triplet_matrix(dtm_data[labels==0,]), var)\n",
    "\n",
    "n1 = sum(labels); \n",
    "n = length(labels)\n",
    "n0 = n - n1\n",
    "\n",
    "myp = (summ[, 1] - summ[, 3]) / sqrt(summ[, 2] / n1 + summ[, 4] / n0)\n",
    "id = order(abs(myp), decreasing=TRUE)[1:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312462d7",
   "metadata": {},
   "source": [
    "There could potentially be some words that are left out of the above selection of words, but could still be useful and aid the interpretability. Here, we will also consider words that never appeared in the positive reviews and similarly, those that never appeared in the negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "025d785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id1 = which(summ[, 2] == 0)\n",
    "id0 = which(summ[, 4] == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb06c388",
   "metadata": {},
   "source": [
    "Then, our chosen vocabulary would be the union of the above selections, `id`, `id0`, and `id1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2e9662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = colnames(dtm_data)\n",
    "myvocab = words[union(id1, union(id, id0))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f41087",
   "metadata": {},
   "source": [
    "However, at this point, the chosen vocabulary has a size `>= 2000`. But, we can still perform additional selections to reduce the size even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b942b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "2006"
      ],
      "text/latex": [
       "2006"
      ],
      "text/markdown": [
       "2006"
      ],
      "text/plain": [
       "[1] 2006"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length(myvocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42378a7a",
   "metadata": {},
   "source": [
    "## Size Reduction using Lasso\n",
    "\n",
    "We can utilize Lasso (with logistic regression) as a variable selector to reduce the size of our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fab3f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "it_data = itoken(data$review,\n",
    "                 preprocessor = tolower,\n",
    "                 tokenizer = word_tokenizer)\n",
    "dtm_data = create_dtm(it_data, vocab_vectorizer(create_vocabulary(myvocab, ngram = c(1L, 4L))))\n",
    "\n",
    "tmpfit = glmnet(x = dtm_data,\n",
    "                y = data$sentiment, \n",
    "                alpha = 1,\n",
    "                family='binomial')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631ddfe4",
   "metadata": {},
   "source": [
    "The `glmnet` output `tmpfit` contains 98 sets of estimated $\\beta$ values corresponding to 98 different lambda values. In particular, `tmpfit$df` tells us the number of non-zero $\\beta$ values (i.e., `df`) for each of the 98 estimates. Since we are interested in a vocabulary size of less than 1K, I chose the largest `df` among those less than 1K (here, the 42nd column) and store the corresponding (here, 983) words in `myvocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92c62ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "42"
      ],
      "text/latex": [
       "42"
      ],
      "text/markdown": [
       "42"
      ],
      "text/plain": [
       "[1] 42"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "983"
      ],
      "text/latex": [
       "983"
      ],
      "text/markdown": [
       "983"
      ],
      "text/plain": [
       "[1] 983"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "largest_idx = length(tmpfit$df[tmpfit$df < 1000])\n",
    "largest_idx\n",
    "\n",
    "myvocab = colnames(dtm_data)[which(tmpfit$beta[, largest_idx] != 0)]\n",
    "length(myvocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f42af",
   "metadata": {},
   "source": [
    "## Save Vocabulary to File\n",
    "\n",
    "Now, let's save the final vocabulary to file (`myvocab.txt`), with each word in the vocabulary saved on a separate line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8d831ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "write.table(myvocab, file = \"myvocab.txt\",\n",
    "            quote = FALSE, row.names = FALSE, col.names = FALSE,\n",
    "            sep = \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
